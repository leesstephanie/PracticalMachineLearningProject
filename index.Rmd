---
title: "WLE_classification"
author: "Stephanie_Lee_S"
date: "3/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Load Data and Take a Look at the Class Distribution

Here, we load the train and the test data set,
```{r}
library(caret)
train_and_test = read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv')
val_set = read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv')
```


and then delete variables from `train_and_test` data set. The deleted variables are variables which has no data at all in the validation data set. This is my first step of preprocess the data sets. The resulting data sets now have 60 variables.
```{r deleting variables in train_and_test}
for (i in 1:length(names(val_set))){
    if (class(val_set[,i])=='logical'){
        val_set[,i] <- as.numeric(val_set[,i])
    }
} #converting any logical variables into numeric

nacols <- colnames(val_set)[apply(val_set, 2, anyNA)]
train_and_test <- train_and_test[, !(names(train_and_test) %in% nacols)]
val_set <- val_set[, !(names(val_set) %in% nacols)]
```


Next, I split `train_and_test` data frame into `training_set` and `testing_set`.
```{r splitting train data}
set.seed(29123)
inTrain = createDataPartition(train_and_test$classe, p = .75,list = F)
training_set = train_and_test[inTrain,]
testing_set = train_and_test[-inTrain,]
```

```{r}
freq <- round(prop.table(table(training_set$classe)), 2)
plot <- plot(training_set$classe, col = 'cornflowerblue',
             ylab = 'frequency', main = 'Class distribution in the training_set')
text(plot, table(training_set$classe), labels = freq, pos = 1)
```

Next, we look the class distribution from the `training_set`. This graph shows a slight class imbalance. While around `r 100*table(training_set$classe)[4]` percent of the observations in `training_set` is labeled D, 28 percent of all observations are labeled as A. However, since the imbalance is not severe, we are going to ignore this class imbalance and not doing any subsampling.



## Further Data Preprocessing

Now, we are going to do more preprocessing. Firstly, I drop any near zero variance variables (which in this case there is only one such variable out of 60 remaining variables). Secondly, I drop the first 7 variables, which are `X`, `user_name`, and other information regarding to the participant themselves. These variables are irrelevant to the quality of exercise itself, so we can remove them. 
Now we have 53 features selected to be included in the model training. The same steps for processing the `training_set` would be applied to `testing_set` and `val_set` as well. 
```{r preprocess}
zv <- nearZeroVar(training_set)
training_set <- training_set[, -c(1:7,zv)] #removing near zero variables
testing_set <- testing_set[, -c(1:7,zv)]
val_set <- val_set[,-c(1:7,zv)]
```



## Training Models

Before we train any model, we need to setup the how `train()` in caret fit models to make training goes faster. I use 5-fold cross validation to train models.
```{r}
trControl <- trainControl(method = 'cv', number = 5, allowParallel = T)
```


### Models Used in this Analysis

I used four models to train data, including k-nearest neighbors, boosting, bagging, and random forest. The predictions from these models would be combined together, and then I use majority vote to decide the classification. In training models, I use all the default tuning parameter, except for how R train the models. The final model is chosen based on the accuracy.



#### K Nearest Neighbors
```{r knn}
set.seed(8954)
model_knn = train(classe~., data = training_set, method = 'knn',
                  trControl = trControl)
model_knn
```



#### Boosting
```{r gbm}
set.seed(9045)
model_gbm = train(classe~., data = training_set, method = 'gbm',
                  trControl = trControl, verbose = F)
model_gbm
```



#### Bagging
```{r bagging}
set.seed(9090)
model_bag = train(classe~., data = training_set, method = 'treebag', 
                  trControl = trControl)
model_bag
```



#### Random Forest
```{r rf}
set.seed(120552)
model_rf = train(classe~., data = training_set, method = 'rf',
                 trControl = trControl)
model_rf
```



### Stacking Models and Majority Vote

The next step is voting, and we are going to see how good these classifiers are in predicting the categories in weight lifting exercises. Firstly, I predict each model on the testing_set, then put it together in a data frame. Since we have four models, we have four variables of `preds`.
```{r}
preds <- data.frame(knn = predict(model_knn, testing_set[, -59]),
                    gbm = predict(model_gbm, testing_set[, -59]),
                    bag = predict(model_bag, testing_set[, -59]), 
                    rf = predict(model_rf, testing_set[, -59]))
```


Secondly, I make a function `majority()` to look for the majority vote. I use `which.max()` function with the default of returning the first value with the majority vote. The function `majority()` would be applied to each row in `preds`.
```{r}
majority <- function(x, vote = c()){
    row = as.factor(x)
    classes <- levels(row)
    vote <- classes[which.max(table(row))]
    vote
}
```

```{r}
finalVote <- apply(preds, 1, majority)
finalVote <- as.factor(finalVote)
confusionMatrix(finalVote, testing_set$classe)
```
We can see here that the accuracy of combined classifiers is `r confusionMatrix(finalVote, testing_set$classe)$overall[1] * 100` percent. This is the out of sample accuracy (remember that accuracy is a way to assess model performance on testing and validation data sets, especially in classification problems). We are going to use the same voting system to predict the labels in the validation data set.



## Prediction

```{r}
predtest_knn = predict(model_knn, val_set[,-59])
predtest_gbm = predict(model_gbm, val_set[,-59])
predtest_bag = predict(model_bag, val_set[,-59])
predtest_rf = predict(model_rf, val_set[,-59])
predstest = data.frame(knn = predtest_knn,
                       gbm = predtest_gbm,
                       bag = predtest_bag,
                       rf = predtest_rf)
#apply(predstest, 1, majority) #the answer to the prediction quiz
```
